# -*- coding: utf-8 -*-
"""AI-Based-Resume-Screening-for-Job-Roles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xx099uX_EKTj456hWSASAoQfX89ZlQP6
"""

# =====================================================================
# üß† AI-Based Resume Screening with Novel Features + BERT + Explainability
# =====================================================================

!pip install -q sentence-transformers transformers xgboost scikit-learn pandas matplotlib seaborn PyPDF2 textstat nltk

import pandas as pd
import numpy as np
import re
import nltk
import textstat
import io
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sentence_transformers import SentenceTransformer
from google.colab import files
from PyPDF2 import PdfReader
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# =====================================================================
# üìÇ STEP 1: Load Dataset (Kaggle Job Descriptions + Resume Dataset)
# =====================================================================
print("Please upload your resume-job dataset (CSV with 'Resume' and 'JobRole' columns):")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
data = pd.read_csv(io.BytesIO(uploaded[file_name]))
print("‚úÖ Dataset loaded:", data.shape)

data = data.dropna().reset_index(drop=True)
data = data.sample(frac=1).reset_index(drop=True)

# =====================================================================
# üì• STEP 2A: Optional O*NET Skill Upload
# =====================================================================
print("\nüìÇ Upload O*NET 'Skills.txt' or 'Skills.csv' (optional)")
skills_df = None
try:
    uploaded_skills = files.upload()
    if len(uploaded_skills) > 0:
        file_name_sk = list(uploaded_skills.keys())[0]
        print(f"‚úÖ Uploaded O*NET file: {file_name_sk}")
        try:
            skills_df = pd.read_csv(io.BytesIO(uploaded_skills[file_name_sk]), sep='\t', on_bad_lines='skip')
        except:
            skills_df = pd.read_csv(io.BytesIO(uploaded_skills[file_name_sk]), on_bad_lines='skip')
        print(f"‚úÖ Loaded {skills_df.shape[0]} O*NET skills.\n")
except Exception as e:
    print(f"‚ö†Ô∏è Skipping O*NET upload: {e}")
    print("Using default skill dictionary.\n")

# =====================================================================
# ‚öôÔ∏è STEP 3: Advanced Feature Extractor (With Your 4 Novel Features)
# =====================================================================
class AdvancedFeatureExtractor:
    def __init__(self):
        self.skill_categories = {
            'programming': ['python', 'java', 'c++', 'c#', 'javascript', 'typescript'],
            'web': ['html', 'css', 'react', 'node', 'angular', 'django', 'flask'],
            'data_science': ['machine learning', 'deep learning', 'pytorch', 'tensorflow', 'numpy', 'pandas', 'scikit-learn'],
            'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform'],
            'database': ['sql', 'mysql', 'mongodb', 'postgresql', 'redis']
        }
        self.soft_skills = ['leadership', 'teamwork', 'communication', 'problem solving', 'creativity', 'analytical']

    # NOVEL FEATURE 1: Experience Level Extraction
    def extract_experience_years(self, text):
        matches = re.findall(r'(\d+)\+?\s*(?:years?|yrs?)', text.lower())
        if matches:
            years = np.mean([int(m) for m in matches])
            return min(years, 30)
        return 0

    # NOVEL FEATURE 2: Comprehensive Skill Extraction
    def extract_skills(self, text):
        text = text.lower()
        skills_count = {}
        for cat, keywords in self.skill_categories.items():
            found = [kw for kw in keywords if kw in text]
            skills_count[f'skills_{cat}'] = len(found)
        skills_count['soft_skills_count'] = sum([1 for s in self.soft_skills if s in text])
        return skills_count

    # NOVEL FEATURE 3: Resume Quality Assessment
    def resume_quality(self, text):
        word_count = len(text.split())
        readability = textstat.flesch_reading_ease(text)
        structure_score = np.clip(word_count / 500, 0, 10)
        quality_score = (readability / 10 + structure_score) / 2
        return np.clip(quality_score, 0, 10)

    # NOVEL FEATURE 4: Comprehensive Candidate Assessment
    def comprehensive_score(self, experience, skills, quality):
        skill_total = sum(skills.values())
        score = (experience * 0.4 + skill_total * 0.4 + quality * 0.2)
        return round(score, 2)

    def extract_all_features(self, text):
        f = {}
        exp = self.extract_experience_years(text)
        skills = self.extract_skills(text)
        quality = self.resume_quality(text)
        f.update(skills)
        f['experience_years'] = exp
        f['resume_quality'] = quality
        f['candidate_score'] = self.comprehensive_score(exp, skills, quality)
        return f

extractor = AdvancedFeatureExtractor()

# =====================================================================
# üî† STEP 4: BERT Embeddings + Feature Fusion
# =====================================================================
model_bert = SentenceTransformer('all-MiniLM-L6-v2')

def preprocess_text(text):
    return re.sub(r'[^a-zA-Z0-9 ]', ' ', text.lower())

data['Cleaned'] = data['Resume'].apply(preprocess_text)
X_bert = model_bert.encode(data['Cleaned'].tolist(), show_progress_bar=True)
bert_df = pd.DataFrame(X_bert)

feature_df = data['Cleaned'].apply(lambda x: pd.Series(extractor.extract_all_features(x)))
X_full = pd.concat([bert_df, feature_df], axis=1)

le = LabelEncoder()
y = le.fit_transform(data['Category'])

X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42, stratify=y)

# =====================================================================
# üß© STEP 5: Model Training (XGBoost)
# =====================================================================
xgb = XGBClassifier(n_estimators=200, max_depth=8, learning_rate=0.1, eval_metric='mlogloss')
xgb.fit(X_train, y_train)

# =====================================================================
# üìä STEP 6: Evaluation
# =====================================================================
y_pred = xgb.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"\n‚úÖ Model Accuracy: {acc*100:.2f}%\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

# Feature importance (Top 15)
importances = pd.Series(xgb.feature_importances_[-len(feature_df.columns):], index=feature_df.columns)
importances.nlargest(15).plot(kind='barh', figsize=(8,5), title='Feature Importance (Top 15)')
plt.show()

# =====================================================================
# üìÑ STEP 7: Test on a Resume PDF
# =====================================================================
print("\nüì• Upload a test resume PDF to evaluate:")
uploaded_resume = files.upload()
resume_file = list(uploaded_resume.keys())[0]

reader = PdfReader(io.BytesIO(uploaded_resume[resume_file]))
resume_text = " ".join([p.extract_text() for p in reader.pages if p.extract_text()])

features = extractor.extract_all_features(resume_text)
bert_vec = model_bert.encode([resume_text])
final_vec = np.concatenate([bert_vec[0], list(features.values())]).reshape(1, -1)

pred_role = le.inverse_transform(xgb.predict(final_vec))[0]

print("\nüß† Predicted Job Role:", pred_role)
print("üìä Candidate Assessment Features:")
for k, v in features.items():
    print(f"  {k}: {v}")

from sklearn.metrics import classification_report, confusion_matrix

y_pred = model_role.predict(X_test)
print(classification_report(y_test, y_pred))

from google.colab import drive
drive.mount('/content/drive')

import joblib
import json

# Save your trained model
joblib.dump(xgb, "/content/xgb_model.pkl")

# Save label encoder
joblib.dump(le, "/content/label_encoder.pkl")

# Save feature extractor configuration (optional)
with open("/content/feature_config.json", "w") as f:
    json.dump(extractor.skill_categories, f, indent=4)

# Save accuracy and evaluation results
results = {
    "accuracy": float(acc),
    "class_report": classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)
}
with open("/content/model_results.json", "w") as f:
    json.dump(results, f, indent=4)

print("‚úÖ All outputs saved in /content/")

# Commented out IPython magic to ensure Python compatibility.
# Step 1: configure Git
!git config --global user.name "shubhiagarwal479"
!git config --global user.email "shubhi.agarwalcs01@gmail.com"

# Step 2: clone your repo
!git clone https://github.com/your-username/your-repo-name.git
# %cd AI-Based-Resume-Screening-for-Job-Roles

# Step 3: copy files into repo
!cp /content/xgb_model.pkl .
!cp /content/label_encoder.pkl .
!cp /content/feature_config.json .
!cp /content/model_results.json .
!cp /content/Resume_JobRole_Classifier.ipynb .

# Step 4: push to GitHub
!git add .
!git commit -m "Added trained XGBoost model and outputs"
!git push https://github.com/your-username/your-repo-name.git main

from google.colab import drive
drive.mount('/content/drive')

save_path = "/content/drive/MyDrive/ML_Project_Files"
import os
os.makedirs(save_path, exist_ok=True)

import joblib, json
joblib.dump(xgb, f"{save_path}/xgb_model.pkl")
joblib.dump(le, f"{save_path}/label_encoder.pkl")

with open(f"{save_path}/feature_config.json", "w") as f:
    json.dump(extractor.skill_categories, f, indent=4)

results = {
    "accuracy": float(acc),
    "class_report": classification_report(
        y_test, y_pred, target_names=le.classes_, output_dict=True
    )
}
with open(f"{save_path}/model_results.json", "w") as f:
    json.dump(results, f, indent=4)

print("‚úÖ All files saved in your Google Drive under 'MyDrive/ML_Project_Files'")

# Commented out IPython magic to ensure Python compatibility.
# ü™Ñ Step 1: Configure your GitHub identity
!git config --global user.name "shubhiagarwal479"
!git config --global user.email "shubhi.agarwal01@gmail.com"

# ü™Ñ Step 2: Clone your GitHub repository into Colab
!git clone https://github.com/shubhiagarwal479/AI-Based-Resume-Screening-for-Job-Roles
# %cd AI-Based-Resume-Screening-for-Job-Roles

# ü™Ñ Step 3: Copy all saved files from Google Drive into repo
!cp /content/drive/MyDrive/ML_Project_Files/* .

# ü™Ñ Step 4: Commit and push to GitHub
!git add .
!git commit -m "Added trained XGBoost model, encoder, and results from Colab"
!git push https://shubhiagarwal479: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@github.com/shubhiagarwal479/AI-Based-Resume-Screening-for-Job-Roles.git main

